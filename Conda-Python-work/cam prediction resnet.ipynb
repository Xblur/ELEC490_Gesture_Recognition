{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "257b6bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import random\n",
    "import json\n",
    "import cv2\n",
    "import math\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53ca7d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"labeled_images.h5\"\n",
    "\n",
    "base_dir = \"./\"+file_name\n",
    "\n",
    "with open(\"labaled_data.json\", \"r\") as outfile:\n",
    "    pointcloud_data = json.load(outfile)\n",
    "\n",
    "classes = set()\n",
    "for i in pointcloud_data:\n",
    "    classes.add(pointcloud_data.get(i)[1])\n",
    "classes = list(classes)\n",
    "\n",
    "\n",
    "SPLIT_RATIO = 0.8\n",
    "with h5py.File(file_name, \"r\") as file:\n",
    "    c = list(zip(list(file['images']),list(file['labels'])))\n",
    "    sz = len(c)\n",
    "    cut = int(sz*SPLIT_RATIO)\n",
    "    random.shuffle(c)\n",
    "\n",
    "    training=c[:cut]\n",
    "    testing=c[cut:]\n",
    "    \n",
    "    training_data=[]\n",
    "    training_labels=[]\n",
    "    testing_data=[]\n",
    "    testing_labels=[]\n",
    "\n",
    "    for a in training:\n",
    "        training_data.append(a[0])\n",
    "        training_labels.append(a[1])\n",
    "\n",
    "    for a in testing:\n",
    "        testing_data.append(a[0])\n",
    "        testing_labels.append(a[1])\n",
    "\n",
    "training_x=np.array(training_data)\n",
    "training_labels=np.array(training_labels)\n",
    "training_data = tf.data.Dataset.from_tensor_slices((training_x, training_labels))\n",
    "testing_x=np.array(testing_data)\n",
    "testing_labels=np.array(testing_labels)\n",
    "testing_data = tf.data.Dataset.from_tensor_slices((testing_x, testing_labels))\n",
    "\n",
    "IMAGE_SIZE = 224\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "input_shape = training_x.shape[1:]\n",
    "\n",
    "train_dataset = training_data.batch(BATCH_SIZE)\n",
    "inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "\n",
    "x = layers.Rescaling(1.0 / 255)(inputs)  # Rescale inputs\n",
    "base_model = keras.applications.ResNet50V2(  # Add the rest of the model\n",
    "    weights=None, input_shape=input_shape, classes=2\n",
    ")(x)\n",
    "\n",
    "\n",
    "model = keras.Model(inputs, base_model)\n",
    "model.compile(optimizer=keras.optimizers.experimental.RMSprop(learning_rate=0.001), \n",
    "              loss=\"sparse_categorical_crossentropy\", \n",
    "              metrics=[keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f51fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    model.load_weights(\"./Saved_resnet_weights\")\n",
    "except:\n",
    "    print(\"no model data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa648973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "183aabb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Processing and model code\n",
    "# import necessary packages\n",
    "\n",
    "# initialize mediapipe\n",
    "mpHands = mp.solutions.hands\n",
    "mpFace = mp.solutions.face_detection\n",
    "hands = mpHands.Hands(max_num_hands=2, min_detection_confidence=0.7)\n",
    "mpDraw = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "\n",
    "# Load the gesture recognizer model\n",
    "# model = load_model('mp_hand_gesture')\n",
    "\n",
    "# Load class names\n",
    "# f = open('gesture.names', 'r')\n",
    "# classNames = f.read().split('\\n')\n",
    "# f.close()\n",
    "# print(classNames)\n",
    "\n",
    "classNames = classes\n",
    "EXTRA_PADDING=0.02\n",
    "\n",
    "# Initialize the webcam\n",
    "drawing_spec = mpDraw.DrawingSpec(thickness=1, circle_radius=1)\n",
    "cap = cv2.VideoCapture(0)\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "    while cap.isOpened():\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if not success:\n",
    "          print(\"Ignoring empty camera frame.\")\n",
    "          # If loading a video, use 'break' instead of 'continue'.\n",
    "          continue\n",
    "        # Read each frame from the webcam\n",
    "\n",
    "#         display(frame)\n",
    "        x, y, c = frame.shape\n",
    "        dimensions = frame.shape\n",
    "        height = frame.shape[0]\n",
    "        width = frame.shape[1]\n",
    "        channels = frame.shape[2]\n",
    "        # Flip the frame vertically\n",
    "        frame.flags.writeable = False\n",
    "        frame = cv2.flip(frame, 2)\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Get hand landmark prediction\n",
    "        result = hands.process(frame)\n",
    "        resultFace = face_mesh.process(frame)\n",
    "        # print(result)\n",
    "\n",
    "        className = ''\n",
    "\n",
    "        # post process the result\n",
    "        frame.flags.writeable = True\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "#         if result.multi_hand_landmarks:\n",
    "#             landmarks = []\n",
    "#             for handslms in result.multi_hand_landmarks:\n",
    "# #                 count +=1\n",
    "#                 for lm in handslms.landmark:\n",
    "#                     # print(id, lm)\n",
    "#                     lmx = int(lm.x * x)\n",
    "#                     lmy = int(lm.y * y)\n",
    "\n",
    "#                     landmarks.append([lmx, lmy])\n",
    "                    \n",
    "\n",
    "#                 # Drawing landmarks on frames\n",
    "#                 mpDraw.draw_landmarks(frame, handslms, mpHands.HAND_CONNECTIONS)\n",
    "#                 if (count <= 1):\n",
    "#                     # Predict gesture\n",
    "#                     prediction = model.predict([landmarks])\n",
    "#                     # print(prediction)\n",
    "#                     classID = np.argmax(prediction)\n",
    "#                     className = classNames[classID]\n",
    "#                 print(\"hands landmarks: \")\n",
    "#                 display(handslms)\n",
    "#         478 face mesh points\n",
    "        if resultFace.multi_face_landmarks:\n",
    "            for face_landmarks in resultFace.multi_face_landmarks:\n",
    "#                 mpDraw.draw_landmarks(\n",
    "#                     image=frame,\n",
    "#                     landmark_list=face_landmarks,\n",
    "#                     connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "#                     landmark_drawing_spec=None,\n",
    "#                     connection_drawing_spec=mp_drawing_styles\n",
    "#                     .get_default_face_mesh_tesselation_style())\n",
    "#                 mpDraw.draw_landmarks(\n",
    "#                     image=frame,\n",
    "#                     landmark_list=face_landmarks,\n",
    "#                     connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "#                     landmark_drawing_spec=None,\n",
    "#                     connection_drawing_spec=mp_drawing_styles\n",
    "#                     .get_default_face_mesh_contours_style())\n",
    "#                 mpDraw.draw_landmarks(\n",
    "#                     image=frame,\n",
    "#                     landmark_list=face_landmarks,\n",
    "#                     connections=mp_face_mesh.FACEMESH_IRISES,\n",
    "#                     landmark_drawing_spec=None,\n",
    "#                     connection_drawing_spec=mp_drawing_styles\n",
    "#                     .get_default_face_mesh_iris_connections_style())\n",
    "#             print(\"face landmarks: \")\n",
    "#             display(face_landmarks)\n",
    "#             current_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "                xyz = [(lm.x, lm.y, lm.z) for lm in face_landmarks.landmark]\n",
    "                points = np.array(xyz)\n",
    "                min_x = (min(points[:, 0])-EXTRA_PADDING) if (min(points[:, 0])-EXTRA_PADDING) > 0 else 0\n",
    "                max_x = (max(points[:, 0])+EXTRA_PADDING) if (min(points[:, 0])+EXTRA_PADDING) < 1 else 1\n",
    "                min_y = (min(points[:, 1])-EXTRA_PADDING) if (min(points[:, 0])-EXTRA_PADDING) > 0 else 0\n",
    "                max_y = (max(points[:, 1])+EXTRA_PADDING) if (min(points[:, 0])+EXTRA_PADDING) < 1 else 1\n",
    "                face_Coor_min_x = int((min_x)*width)\n",
    "                face_Coor_max_x = int((max_x)*width)\n",
    "                face_Coor_min_y = int((min_y)*height)\n",
    "                face_Coor_max_y = int((max_y)*height)\n",
    "                if (face_Coor_max_x-face_Coor_min_x) < (face_Coor_max_y-face_Coor_min_y):\n",
    "                    face_Coor_max_x=math.ceil((face_Coor_max_x+face_Coor_min_x)/2)+math.ceil((face_Coor_max_y-face_Coor_min_y)/2)\n",
    "                    face_Coor_min_x=math.floor((face_Coor_max_x+face_Coor_min_x)/2)-math.ceil((face_Coor_max_y-face_Coor_min_y)/2)\n",
    "                    if face_Coor_min_x < 0 :\n",
    "                        face_Coor_min_x = 0\n",
    "                    if face_Coor_max_x > width:\n",
    "                        face_Coor_max_x = width\n",
    "                elif (face_Coor_max_x-face_Coor_min_x) > (face_Coor_max_y-face_Coor_min_y):\n",
    "                    face_Coor_max_y=math.ceil((face_Coor_max_y+face_Coor_min_y)/2)+math.ceil((face_Coor_max_x-face_Coor_min_x)/2)\n",
    "                    face_Coor_min_y=math.floor((face_Coor_max_y+face_Coor_min_y)/2)-math.ceil((face_Coor_max_x-face_Coor_min_x)/2)\n",
    "                    if face_Coor_min_y < 0 :\n",
    "                        face_Coor_min_y = 0\n",
    "                    if face_Coor_max_y > height:\n",
    "                        face_Coor_max_y = height\n",
    "                count =0\n",
    "                while (face_Coor_max_x-face_Coor_min_x) != (face_Coor_max_y-face_Coor_min_y):\n",
    "                    if (face_Coor_max_x-face_Coor_min_x) < (face_Coor_max_y-face_Coor_min_y):\n",
    "                        if count%2==0:\n",
    "                            face_Coor_max_x+=1\n",
    "                        elif count%2==1:\n",
    "                            face_Coor_min_x-=1\n",
    "                    elif (face_Coor_max_x-face_Coor_min_x) > (face_Coor_max_y-face_Coor_min_y):\n",
    "                        if count%2==0:\n",
    "                            face_Coor_max_y+=1\n",
    "                        elif count%2==1:\n",
    "                            face_Coor_min_y-=1\n",
    "                    count+=1\n",
    "\n",
    "                face = frame[face_Coor_min_y:face_Coor_max_y, face_Coor_min_x:face_Coor_max_x]\n",
    "\n",
    "                dim = (224, 224)\n",
    "                resized = cv2.resize(face, dim, interpolation = cv2.INTER_AREA)\n",
    "                face=np.array([resized])\n",
    "    #             display(\"face image data shape:\", resized.shape)\n",
    "                # Predict gesture\n",
    "                prediction = model.predict(face[:1], verbose='0')\n",
    "                # print(prediction)\n",
    "                classID = np.argmax(prediction)\n",
    "                className = classNames[classID]\n",
    "#             face_lm_inputs = \n",
    "#             display(xyzt)\n",
    "            \n",
    "#         time.sleep(5)\n",
    "            \n",
    "        # show the prediction on the frame\n",
    "        frame = cv2.flip(frame, 1)\n",
    "        cv2.putText(frame, className, (10, 50), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                       1, (0,0,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show the final output\n",
    "        cv2.imshow(\"Output\", frame) \n",
    "\n",
    "        if cv2.waitKey(1) == ord('q'):\n",
    "            break\n",
    "\n",
    "# release the webcam and destroy all active windows\n",
    "cap.release()\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51b1069",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72bb0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
